{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System libs\n",
    "import os\n",
    "import argparse\n",
    "from distutils.version import LooseVersion\n",
    "# Numerical libs\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.io import loadmat\n",
    "import csv\n",
    "# Our libs\n",
    "from mit_semseg.dataset import TestDataset\n",
    "from mit_semseg.models import ModelBuilder, SegmentationModule\n",
    "from mit_semseg.utils import colorEncode, find_recursive, setup_logger\n",
    "from mit_semseg.lib.nn import user_scattered_collate, async_copy_to\n",
    "from mit_semseg.lib.utils import as_numpy\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from mit_semseg.config import cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## semantic segamentation based painting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {}\n",
    "with open('data/object150_info.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        names[int(row[0])] = row[5].split(\";\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = loadmat('data/color150.mat')['colors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_result(data, pred, cfg):\n",
    "    (img, info) = data\n",
    "\n",
    "    # print predictions in descending order\n",
    "    pred = np.int32(pred)\n",
    "    pixs = pred.size\n",
    "    uniques, counts = np.unique(pred, return_counts=True)\n",
    "    print(\"Predictions in [{}]:\".format(info))\n",
    "    for idx in np.argsort(counts)[::-1]:\n",
    "        name = names[uniques[idx] + 1]\n",
    "        ratio = counts[idx] / pixs * 100\n",
    "        if ratio > 0.1:\n",
    "            print(\"  {}: {:.2f}%\".format(name, ratio))\n",
    "\n",
    "    # colorize prediction\n",
    "    pred_color = colorEncode(pred, colors).astype(np.uint8)\n",
    "\n",
    "    # aggregate images and save\n",
    "    im_vis = np.concatenate((img, pred_color), axis=1)\n",
    "\n",
    "    img_name = info.split('/')[-1]\n",
    "    Image.fromarray(im_vis).save(\n",
    "        os.path.join(cfg.TEST.result, img_name.replace('.jpg', '.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(segmentation_module, loader, gpu):\n",
    "    segmentation_module.eval()\n",
    "\n",
    "    pbar = tqdm(total=len(loader))\n",
    "    for batch_data in loader:\n",
    "        # process data\n",
    "        batch_data = batch_data[0]\n",
    "        segSize = (batch_data['img_ori'].shape[0],\n",
    "                   batch_data['img_ori'].shape[1])\n",
    "        img_resized_list = batch_data['img_data']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            scores = torch.zeros(1, cfg.DATASET.num_class, segSize[0], segSize[1])\n",
    "            scores = async_copy_to(scores, gpu)\n",
    "\n",
    "            for img in img_resized_list:\n",
    "                feed_dict = batch_data.copy()\n",
    "                feed_dict['img_data'] = img\n",
    "                del feed_dict['img_ori']\n",
    "                del feed_dict['info']\n",
    "                feed_dict = async_copy_to(feed_dict, gpu)\n",
    "\n",
    "                # forward pass\n",
    "                pred_tmp = segmentation_module(feed_dict, segSize=segSize)\n",
    "                scores = scores + pred_tmp / len(cfg.DATASET.imgSizes)\n",
    "\n",
    "            _, pred = torch.max(scores, dim=1)\n",
    "            pred = as_numpy(pred.squeeze(0).cpu())\n",
    "            print(\"pred:\", pred)\n",
    "\n",
    "        # visualization\n",
    "        visualize_result(\n",
    "            (batch_data['img_ori'], batch_data['info']),\n",
    "            pred,\n",
    "            cfg\n",
    "        )\n",
    "\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(imglist, select_model_option=\"ade20k-hrnetv2\" ): # select_model_option = \"ade20k-mobilenetv2dilated-c1_deepsup\"\n",
    "    cfg.merge_from_file(\"config/\" + select_model_option + \".yaml\")\n",
    "\n",
    "    logger = setup_logger(distributed_rank=0)   # TODO\n",
    "#     logger.info(\"Loaded configuration file {}\".format(\"config/\" + select_model_option + \".yaml\"))\n",
    "#     logger.info(\"Running with config:\\n{}\".format(cfg))\n",
    "\n",
    "    cfg.MODEL.arch_encoder = cfg.MODEL.arch_encoder.lower()\n",
    "    cfg.MODEL.arch_decoder = cfg.MODEL.arch_decoder.lower()\n",
    "\n",
    "    # absolute paths of model weights\n",
    "    cfg.MODEL.weights_encoder = os.path.join(\n",
    "        cfg.DIR, 'encoder_' + cfg.TEST.checkpoint)\n",
    "    cfg.MODEL.weights_decoder = os.path.join(\n",
    "        cfg.DIR, 'decoder_' + cfg.TEST.checkpoint)\n",
    "\n",
    "    assert os.path.exists(cfg.MODEL.weights_encoder) and \\\n",
    "        os.path.exists(cfg.MODEL.weights_decoder), \"checkpoint does not exitst!\"\n",
    "\n",
    "    # generate testing image list\n",
    "    if os.path.isdir(imglist):\n",
    "        imgs = find_recursive(imglist)\n",
    "    else:\n",
    "        imgs = [imglist]\n",
    "    assert len(imgs), \"imgs should be a path to image (.jpg) or directory.\"\n",
    "    cfg.list_test = [{'fpath_img': x} for x in imgs]\n",
    "\n",
    "    if not os.path.isdir(cfg.TEST.result):\n",
    "        os.makedirs(cfg.TEST.result)\n",
    "\n",
    "    gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    torch.cuda.set_device(gpu)\n",
    "\n",
    "    # Network Builders\n",
    "    net_encoder = ModelBuilder.build_encoder(\n",
    "        arch=cfg.MODEL.arch_encoder,\n",
    "        fc_dim=cfg.MODEL.fc_dim,\n",
    "        weights=cfg.MODEL.weights_encoder)\n",
    "    net_decoder = ModelBuilder.build_decoder(\n",
    "        arch=cfg.MODEL.arch_decoder,\n",
    "        fc_dim=cfg.MODEL.fc_dim,\n",
    "        num_class=cfg.DATASET.num_class,\n",
    "        weights=cfg.MODEL.weights_decoder,\n",
    "        use_softmax=True)\n",
    "\n",
    "    crit = nn.NLLLoss(ignore_index=-1)\n",
    "\n",
    "    segmentation_module = SegmentationModule(net_encoder, net_decoder, crit)\n",
    "\n",
    "    # Dataset and Loader\n",
    "    dataset_test = TestDataset(\n",
    "        cfg.list_test,\n",
    "        cfg.DATASET)\n",
    "    loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=cfg.TEST.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=user_scattered_collate,\n",
    "        num_workers=5,\n",
    "        drop_last=True)\n",
    "\n",
    "    segmentation_module.cuda()\n",
    "\n",
    "    # Main loop\n",
    "    test(segmentation_module, loader_test, gpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights for net_encoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights for net_decoder\n",
      "# samples: 1\n",
      "pred: [[ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " ...\n",
      " [23 23 23 ... 23 23 23]\n",
      " [23 23 23 ... 23 23 23]\n",
      " [23 23 23 ... 23 23 23]]\n",
      "Predictions in [ADE_val_00001519.jpg]:\n",
      "  wall: 19.90%\n",
      "  sofa: 15.06%\n",
      "  door: 12.44%\n",
      "  ceiling: 11.38%\n",
      "  shelf: 10.89%\n",
      "  floor: 8.57%\n",
      "  coffee: 5.09%\n",
      "  fireplace: 3.91%\n",
      "  cushion: 3.80%\n",
      "  armchair: 2.95%\n",
      "  flower: 1.09%\n",
      "  curtain: 1.07%\n",
      "  windowpane: 0.85%\n",
      "  lamp: 0.71%\n",
      "  table: 0.53%\n",
      "  chair: 0.41%\n",
      "  book: 0.34%\n",
      "  basket: 0.22%\n",
      "  mirror: 0.19%\n",
      "  vase: 0.17%\n",
      "  clock: 0.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n"
     ]
    }
   ],
   "source": [
    "inference(\"ADE_val_00001519.jpg\", select_model_option=\"ade20k-mobilenetv2dilated-c1_deepsup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paintnet",
   "language": "python",
   "name": "paintnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
